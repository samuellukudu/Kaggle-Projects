{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ed929b-763b-4ac3-986e-3c60cc544af9",
   "metadata": {},
   "source": [
    "# HOW TO USE OUT OF FOLD PREDICTIONS IN MACHINE LEARNING\n",
    "\n",
    "Machine Learning Algorithms are typically evaluated using *resampling techniques* such as **K-fold cross-validation** \n",
    "\n",
    "During cross-validation process, predictions are made on test sets comprised of data not used to train the model. These predictions are referred to as **out-of-fold predictions**, a type of out-of-sample predictions\n",
    "\n",
    "Out-of-fold predictions play an important role in ML in both estimating the performance of a model when making predictions on new data in the future, so-called the generalization performance of the model and in the development of ensemble models\n",
    "\n",
    "<li>Out-of-fold predictions are a type of out-of-sample predictions made on data not used to train a model</li>\n",
    "<li>Out-of-fold predictions are mostly used to estimate the performance of the of a model when making predictions on an unseen data</li>\n",
    "<li>Out-of-fold predictions can be used to construct an ensemble model called a stacked generalization or stacking ensemble</li>\n",
    "\n",
    "Two main uses for **out-of-fold predictions**\n",
    "\n",
    "<li>Estimate the performance of the model on unseen data</li>\n",
    "<li>Fit an ensemble model</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78eca5-f453-4685-9d46-4b6ddcd0b555",
   "metadata": {},
   "source": [
    "## Out-of-fold Predictions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb677199-7f50-43ce-b3ed-d6483be2a281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# The example below prepares a data sample and summarizes the shape of the \n",
    "# input and output elements of the dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43dab377-b421-42e0-84d3-ee8084a77056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06331b809e4c499abd1eed37274e2ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.941%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by averaging performance across each fold\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data_y, data_yhat = list(), list()\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "# enumerate splits\n",
    "for fold, (tr_idx, test_idx) in enumerate(tqdm(kfold.split(X), total=10)):\n",
    "    # get the data\n",
    "    train_X, test_X = X[tr_idx], X[test_idx]\n",
    "    train_y, test_y = y[tr_idx], y[test_idx]\n",
    "    # fit the model\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(train_X, train_y)\n",
    "    # evaluate the model\n",
    "    yhat = model.predict(test_X)\n",
    "    # store\n",
    "    data_y.extend(test_y)\n",
    "    data_yhat.extend(yhat)\n",
    "    \n",
    "# summarize the model performance\n",
    "acc = accuracy_score(data_y, data_yhat)\n",
    "print(f\"Accuracy {acc:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f548ab-8ac5-4206-b53d-c0df4c1c67a1",
   "metadata": {},
   "source": [
    "## Out-of-fold Predictions for Ensembles\n",
    "\n",
    "An ensemble is a machine learning model that combines the predictions from two or more models prepared on the same training dataset\n",
    "\n",
    "Out-of-fold predictions in aggregate provide information about how the model performs on each example in the training dataset when not used to train the model. This information can be used to train a model to correct or improve upon those predictions\n",
    "\n",
    "First, the k-fold cross-validation procedure is performed on each base model of interest, and each out-of-fold predictions are collected. \n",
    "<li><b>Base Models</b>: Models evaluated using k-fold cross-validation on the training dataset and all out-of-fold predictions are retained</li>\n",
    "<li><b>Meta Models</b>: Model that takes the <em>out-of-fold predictions</em> made by one or more models as input and shows how to best combine and correct the predictions</li>\n",
    "<li><b>Meta Model Input</b>: Input portion of a given sample concatenated with the predictions made by each base model</li>\n",
    "<li><b>Meta Model Output</b>: Output portion of a given sample</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5822fb4-dd36-491e-ba08-8af90b085175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0252d745b3644408b3d8223afb5053f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 acc: 0.7152\n",
      "Model 2 acc:  0.924242\n",
      "Meta model accuracy: 0.9576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create a meta dataset\n",
    "def create_meta_dataset(data_x, yhat1, yhat2):\n",
    "    # convert to columns\n",
    "    yhat1 = np.array(yhat1).reshape((len(yhat1), 1))\n",
    "    yhat2 = np.array(yhat2).reshape((len(yhat2), 1))\n",
    "    # stack as separate columns\n",
    "    meta_X = np.hstack((data_x, yhat1, yhat2))\n",
    "    return meta_X\n",
    "\n",
    "\n",
    "# make predictions with stacked model\n",
    "def stack_prediction(model1, model2, meta_model, X):\n",
    "    # make predictions\n",
    "    yhat1 = model1.predict(X)\n",
    "    yhat2 = model2.predict(X)\n",
    "    meta_X = create_meta_dataset(X, yhat1, yhat2)\n",
    "    # predict\n",
    "    return meta_model.predict(meta_X)\n",
    "\n",
    "\n",
    "# create the inputs and outputs\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# split the data\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.33)\n",
    "# collect out of sample predictions\n",
    "data_x, data_y, knn_yhat, cart_yhat = list(), list(), list(), list()\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "for fold, (tr_idx, test_idx) in enumerate(tqdm(kfold.split(X), total=10)):\n",
    "    # get data\n",
    "    train_X, test_X = X[tr_idx], X[test_idx]\n",
    "    train_y, test_y = y[tr_idx], y[test_idx]\n",
    "    data_x.extend(test_X)\n",
    "    data_y.extend(test_y)\n",
    "    # fit and make predictions with cart\n",
    "    model1 = DecisionTreeClassifier()\n",
    "    model1.fit(train_X, train_y)\n",
    "    yhat1 = model1.predict_proba(test_X)[:, 0]\n",
    "    cart_yhat.extend(yhat1)\n",
    "    \n",
    "    # fit and make predictions with knn\n",
    "    model2 = KNeighborsClassifier()\n",
    "    model2.fit(train_X, train_y)\n",
    "    yhat2 = model2.predict_proba(test_X)[:, 0]\n",
    "    knn_yhat.extend(yhat2)\n",
    "    \n",
    "# construct meta dataset\n",
    "meta_X = create_meta_dataset(data_x, knn_yhat, cart_yhat)\n",
    "\n",
    "# fit final ensemble\n",
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(X, y)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(X, y)\n",
    "\n",
    "# construct meta classifier\n",
    "meta_model = LogisticRegression(solver='liblinear')\n",
    "meta_model.fit(meta_X, data_y)\n",
    "\n",
    "# evaluate sub models on hold out dataset\n",
    "acc1 = accuracy_score(y_val, model1.predict(X_val))\n",
    "acc2 = accuracy_score(y_val, model2.predict(X_val))\n",
    "print(f\"Model 1 acc: {acc1:.4f}\\nModel 2 acc: {acc2: 4f}\")\n",
    "\n",
    "# evaluate meta model on hold out dataset\n",
    "yhat = stack_prediction(model1, model2, meta_model, X_val)\n",
    "acc = accuracy_score(y_val, yhat)\n",
    "print(f\"Meta model accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd226f-4062-4edf-930d-3abf445407ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
